from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import glob
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import BaggingRegressor

data = pd.read_csv('ERT/ert_T.csv')

print("DataFrame:")
print(data)

# data.dropna(inplace=True)
# data.to_csv('ERT/ert_T_cleaned.csv', index=False)

df = pd.DataFrame(data)
print(df)
# df.drop(df.columns[[2, 3]], axis=1, inplace=True)
# print(df)
# Drop the first column that is the name of the combinations
df.drop(df.columns[[0]], axis=1, inplace=True)
# Reset index after dropping rows
df.reset_index(drop=True, inplace=True)

# defining X and y
X = df.drop(columns=['T'])  # 'T' is the target
y = df['T']

# Step 3: Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Normalization
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# # Step 6: predictions
# y_pred = rf_regressor.predict(X_test)
#
# # Step 7: Evaluation
# mse = mean_squared_error(y_test, y_pred)
# print(f'Mean Squared Error: {mse}')

# Add Baggings, Single Perceptron and RF
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
bagging_decision_tree = BaggingRegressor(base_estimator=DecisionTreeRegressor(random_state=42), n_estimators=10, random_state=42)
bagging_single_perceptron = BaggingRegressor(base_estimator=MLPRegressor(hidden_layer_sizes=(1,), activation='identity', solver='lbfgs', random_state=42), n_estimators=10, random_state=42)

# Step 6: predictions
models = {
    "Random Forest": rf_regressor,
    # "Linear Regression": LinearRegression(fit_intercept=True),
    # "Ridge Regression": Ridge(alpha=1.0, fit_intercept=True, normalize=False, solver='auto', max_iter=None, tol=0.001, random_state=None),
    # "Lasso Regression": Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True,
    #                     max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None,
    #                     selection='cyclic'),
    # "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42, criterion='friedman_mse',
    #                                                min_samples_split=2, min_samples_leaf=1,  max_features=None),
    # "AdaBoost": AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=4), n_estimators=50, learning_rate=1.0,
    #                                    loss='linear', random_state=None),
    # "Support Vector Machine": SVR(),
    # "K-Nearest Neighbors": KNeighborsRegressor(n_neighbors=2, weights='uniform', algorithm='auto'),
    # "Decision Tree": DecisionTreeRegressor(random_state=42, criterion='mse', splitter='best', max_depth=10, min_samples_split=2,
    #                                        min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=9),
    # "Multi-layer Perceptron": MLPRegressor(max_iter=3000, random_state=42, hidden_layer_sizes=(3000), activation='relu',
    #                                        solver='adam'),
    # "Single-Layer Perceptron": MLPRegressor(hidden_layer_sizes=(1,), activation='identity', solver='lbfgs', random_state=42),
    # "Neural Network": MLPRegressor(hidden_layer_sizes=(10000), activation='relu', solver='lbfgs', max_iter=1000, random_state=42),
    # "Bagging Decision Tree": bagging_decision_tree,
    # "Bagging Single Perceptron": bagging_single_perceptron
}


for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f'{name} MSE: {mse:.5f}')
