import deslib
from deslib.dcs import OLA, MLA
from deslib.des import METADES, KNORAU, DESMI, DESP
from deslib.des.knora_e import KNORAE
from deslib.static import Oracle
from sklearn.metrics import accuracy_score
import glob
import numpy as np
from numpy import mean
import pickle
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import glob
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import BaggingRegressor

data = pd.read_csv('ERT/ert_T.csv')

print("DataFrame:")
print(data)

# data.dropna(inplace=True)
# data.to_csv('ERT/ert_T_cleaned.csv', index=False)

df = pd.DataFrame(data)
print(df)
# df.drop(df.columns[[2, 3]], axis=1, inplace=True)
# print(df)
# Drop the first column that is the name of the combinations
df.drop(df.columns[[0]], axis=1, inplace=True)
# Reset index after dropping rows
df.reset_index(drop=True, inplace=True)

# defining X and y
X = df.drop(columns=['T'])  # 'T' is the target
y = df['T']

# Step 3: Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Normalization
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# # Step 6: predictions
# y_pred = rf_regressor.predict(X_test)
#
# # Step 7: Evaluation
# mse = mean_squared_error(y_test, y_pred)
# print(f'Mean Squared Error: {mse}')

# Add Baggings, Single Perceptron and RF
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
bagging_decision_tree = BaggingRegressor(base_estimator=DecisionTreeRegressor(random_state=42), n_estimators=10, random_state=42)
bagging_single_perceptron = BaggingRegressor(base_estimator=MLPRegressor(hidden_layer_sizes=(1,), activation='identity', solver='lbfgs', random_state=42), n_estimators=10, random_state=42)

# Step 6: predictions
models = {
    "Random Forest": rf_regressor,
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "Lasso Regression": Lasso(),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "AdaBoost": AdaBoostRegressor(random_state=42),
    "Support Vector Machine": SVR(),
    "K-Nearest Neighbors": KNeighborsRegressor(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Multi-layer Perceptron": MLPRegressor(random_state=42),
    "Single-Layer Perceptron": MLPRegressor(hidden_layer_sizes=(1,), activation='identity', solver='lbfgs', random_state=42),
    "Neural Network": MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=42),
    "Bagging Decision Tree": bagging_decision_tree,
    "Bagging Single Perceptron": bagging_single_perceptron
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f'{name} MSE: {mse:.5f}')
